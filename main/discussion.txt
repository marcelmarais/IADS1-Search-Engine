It becomes clear that this implementation rapidly becomes inefficient as large number of keys are used. This is because the number of different combinations that qualify a hit
increase exponentially as more keys are added. This is clearly demonstrated if we for
example search with a lineWindow = 1, minRequired = 2 and keys = [a,b]. The only
option that qualifies a hit is a single line containing both a and b. Now, if we keep everything constant and make keys = [a,b,c] the possible hits are now a single line containing any of the following: [a,b],[a,c],[b,c], or [a,b,c]. It easy is easy to imagine that the list of possible hits grows rapidly. Of course more possible hits means that more
hits have to be searched for. The way this is being done in my implementation is simply a linear search and thus takes roughly O(n) time. If were instead to use some kind of data structure that optimises searching, for example a balanced tree (red black tree) we could speed this up to a O(lg(n)). We could also develop an algorithm that traverses the tree in a manner that each successive traversal alternates between words to optimise for our requirement that keys have to be distinct, this would have the benefit of keeping our approach efficient if minRequired becomes large.